# 데이터 전처리
## 1. 데이터 전처리 : 데이터 생성, 데이터 정제
### 데이터 생성
- 요약변수
    - 수집된 정보를 분석의 목적에 맞게 종합한 변수
    - 많은 모델에 공통으로 사용될 수 있어 재활용성이 높음 \
    (단어빈도, 상품별 구매금액, 상품별 구매량, 영화매출액 등)
- 파생변수
    - 특정한 의미를 갖는 작위적 정의에 의한 변수
    - 사용자가 특정조건을 만족하거나 특정함수에 의해 값을 만들어 의미를 부여한 변수
    - 매우 주관적일 수 있으므로 논리적 타당성을 갖춰야 함\
    (구매상품 다양성 변수, 가격선호대 변수, 라이프스타일 변수, 영화 인기도 변수)

### 데이터 정제
- 결측값 : 기록누락, 미응답, 수집오류 등의 이유로 발생
    - 결측값이 포함된 자료라도 나머지 변수의 값들은 의미있는 정보이므로 정보의 손실을 최소화하도록 결측을 처리하는 것이 바람직함

- 이상값 : 다른 데이터와 동떨어진 것
    - 다른 자료값들에 비해 멀리 떨어져 있지만 의미가 있는 값일 수도 있고 단순히 입력 오류로 발생한 값일 수도 있음

- 변수구간화 : 연속형 변수를 구간을 이용하여 범주화 하는 과정
    - 이상치 문제 완화
    - 결측치 처리방법이 될 수 있음
    - 변수간 관계가 단순화되어 분석시 과적합을 방지하고 결과 해석 용이

#### 결측값 처리법
- 완전제거법
    - 결측값이 하나 이상 포함된 자료를 제거
    - 정보의 손실로 분석 결과가 왜곡될 수 있음

- 평균대체법
    - 결측값을 해당 변수의 나머지값들의 평균으로 대체
    - 추정량의 표준오차가 과소추정되는 문제가 있음

- 핫덱대체법
    - 동일한 데이터 내에서 결측값이 발생하는 관찰치와 유사한 특성을 가진 다른 관찰치의 정보를 이용하여 대체

- 그 외 : Regression imputation, KNN imputation 등

#### 이상값의 탐지
- 상자그림 : Q1 - 1.5 * IQR 과 Q3 + 1.5 * IQR의 범위를 넘어가는 자료를 이상값으로 진단
![Alt text](/reference_machinelearning/image-1.png)

- 표준화 점수(Z-score) : 표준화 점수의 절대값이 2, 3보다 큰 경우를 이상값으로 진단

#### 이상값 처리방법
- 이상값 제외 : 처리는 간단하지만 정보손실이 발생하고 추정량 왜곡이 생길 수 있음
- 이상값 대체 : 이상값을 정상값 중 최대 또는 최소 등으로 대체
- 변수변환 : 자료값 전체에 로그변환, 제곱근 변환 등을 적용


## 2. 데이터 전처리 : 데이터 변환, 데이터 결합
### 데이터 변환
- 자료 변환을 통해 자료의 해석을 쉽고 풍부하게 하기 위한 과정
- 데이터 변환 목적
    - 분포의 대칭화
    - 산포를 비슷하게 하기 위해
    - 변수 간 관계를 단순하게 하기 위해

- 제곱근 변환 : 오른쪽 꼬리가 길 때
- 제곱 변환 : 왼쪽 꼬리가 길 때
![Alt text](/reference_machinelearning/image-2.png)

- 로그 변환 : 오른쪽 꼬리가 길 때(제곱근 변환보다 극대화)
- 지수 변환 : 왼쪽 꼬리가 길 때(제곱 변환보다 극대화)
![Alt text](/reference_machinelearning/image-3.png)

- 박스콕스 변환 
    - 제곱근 유형의 변환을 일반화(왼쪽 그래프)
    - 로그 유형의 변환을 일반화(왼쪽 그래프)
    - 제곱 유형의 변환을 일반화(오른쪽 그래프)
![Alt text](/reference_machinelearning/image-4.png)

### 데이터 결합
- 이너조인 : 두 테이블에 키가 공통으로 존재하는 레코드만 결합
- 풀아우터조인 : 두 테이블 중 어느 한쪽이라도 존재하는 키에 대한 레코드를 모두 결합
- 레프트조인 : 왼쪽 테이블에 존재하는 키에 대한 레코드를 결합
- 라이트조인 : 오른쪽 테이블에 존재하는 키에 대한 레코드를 결합


# 머신러닝의 기초
## 3. 머신러닝의 기본 개념 및 방법론의 분류
- 머신러닝 
    - 컴퓨터 시스탬에 명시적으로 프로그래밍 하지 않더라도 데이터를 스스로 학습하여 문제를 해결할 수 있게 하는 기술
    - 사람이 인지하기 어려운 복잡한 규칙과 패턴을 파악하여 의미있는 결과를 얻을 수 있음

- 머신러닝 알고리즘의 발전, 컴퓨팅 성능의 발전, 대용량 데이터의 축적 및 관리기술 발전으로 머신러닝의 활용증가

### 머신러닝 방법론
- 지도학습
    - 라벨이 있는 훈련용 데이터에서 여러 특성변수를 이용하여 목표변수인 라벨을 예측하도록 모델을 학습
    - 라벨이 연속형이면 회귀알고리즘, 라벨이 범주형이면 분류알고리즘 (회귀 : 가격, 분류 : 생존여부)
    ![Alt text](/reference_machinelearning/image-5.png)

- 비지도학습(자율학습)
    - 라벨이 없는 훈련용 데이터에서 특징 변수들 간의 관계나 유사성을 기반으로 의미있는 패턴을 추출
    - 군집화, 차원축소, 추천시스템 등에 활용
    ![Alt text](/reference_machinelearning/image-6.png)
    ![Alt text](/reference_machinelearning/image-7.png)

- 강화학습
    - 행동하는 주체가 있고 행동을 했을 때의 상태와 보상을 바꿔주는 환경으로 구성
    - 주체가 매번 어떠한 행동을 하면 환경에 의해 상태와 보상이 바뀌면서 주체는 보상이 가장 커지는 방향으로 계속 학습해 나감


## 4. 머신러닝 모델의 검증 및 평가
- 지도학습 알고리즘의 분석 절차
    1. 주어진 데이터 전처리 및 탐색
    2. 적절한 모델을 선택
    3. 주어진 데이터로 모델을 훈련
    4. 훈련된 모델을 적용하여 새로운 데이터에 대한 예측을 수행

- 과대적합 문제
    - 주어진 자료는 거의 완벽한 예측이 가능하지만 미래의 새로운 자료에 대한 예측력이 떨어지는 문제
    - 복잡한 알고리즘을 사용하여 데이터를 훈련하는 경우 과대적합 문제를 항상 염두에 두어야 함
    ![Alt text](/reference_machinelearning/image-8.png)

- 모델 평가의 필요성
    - 과대적합을 막고 일반화 오차를 줄이기 위해 새로운 데이터에 얼마나 잘 일반화될지를 파악해야 함
    - 모델 적합에 사용된 자료를 평가를 위해 재활용하지 않고, 평가만을 위한 데이터를 확보할 필요가 있음

### 모델 검증 및 평가를 위한 데이터의 구분
- Hold-out 방식 : 주어진 자료를 세 그룹으로 분할한 뒤 주어진 목적에 따라 각각 모델의 훈련, 검증, 평가에 활용
    1. 훈련 데이터 : 모델의 학습을 위해 사용되는 자료
    2. 검증 데이터 : 훈련 자료로 적합되는 모델을 최적의 성능으로 튜닝하기 위해 사용되는 자료\
    훈련에 필요한 하이퍼파라미터를 조정하거나 변수선택 등에 이용
    3. 평가 데이터 : 훈련 및 검증자료로 적합된 최종 모형이 미래에 주어질 새로운 자료에 대하여 얼마나 좋은 성과를 갖는지를 평가하는데 사용되는 자료

- K-fold 교차검증 방식 : 자료의 수가 충분하지 않은 경우에는 훈련 데이터에서 너무 많은 양의 데이터를 검증 또는 평가 데이터에 뺏기지 않도록 교차검증기법을 사용
    1. 자료를 균등하게 k개의 그룹으로 분할
    2. 각 j에 대하여 j번째 그룹을 제외한 나머지 k-1개 그룹의 자료를 이용하여 모델을 적합
    3. j번째 그룹의 자료에 적합된 모델을 적용한 뒤 예측 오차를 구함
    4. j = 1, 2, ..., k에 대하여 위의 과정을 반복한 뒤 k개의 예측오차의 평균을 구함
    5. 예측오차의 평균값을 기준으로 모델의 검증 또는 평가를 수행

### 편향-분산 트레이드 오프
- 일반화 오차 = 편향^2 + 분산
- 모델의 복잡한 정도에 따라 훈련데이터와 평가데이터의 예측오차는 아래와 같은 패턴을 보임
![Alt text](/reference_machinelearning/image-9.png)

#### 과대적합을 막기 위한 방법
- 훈련 데이터를 많이 확보
- 모델의 복잡도를 낮춤
    - 특성 변수의 수를 줄이거나 차원축소
    - 파라미터에 규제를 적용


## 5. 머신러닝 모델의 평가지표
### 회귀모델의 평가지표
- RMSE\
![Alt text](/reference_machinelearning/image-10.png)\
오차 제곱의 평균의 제곱근
- R-suare\
![Alt text](/reference_machinelearning/image-11.png)
- MAE : 오차의 부호만 제거해서 이를 평균한 값\
![Alt text](/reference_machinelearning/image-12.png)
- MAPE : 실제값 대비 오차가 차지하는 비중이 평균적으로 얼마인지 확인\
![Alt text](/reference_machinelearning/image-13.png)

### 분류모델의 평가지표
- 정오분류표
![Alt text](/reference_machinelearning/image-14.png)

- 정확도, 정분류율 : 전체 관찰치 중 정분류된 관찰치의 비중\
A + D / (A + B + C + D) = (TN + TP) / (TN + FP + FN + TP)

- 정밀도 : Positive로 예측한 것 중에 실제 범주도 Positive인 데이터의 비율\
(스팸 구분)\
D / (B + D) = TP / (FP + TP)

- 재현율 : 실제 범주가 Positive인 것 중에서 Positive로 예측된 데이터의 비율 \
(암환자 평가)\
D / (C + D) = TP / (FN + TP)

- ROC 도표 : 분류의 결정임계값에 따라 달라지는 TPR과 FPR의 조합
    1. TPR(True Positive Rate) : 1인 케이스에 대해 1로 잘 예측한 비율
    2. FPR(False Positive Rate) : 0인 케이스에 대해 1로 잘못 예측한 비율
    3. 임계값이 1이면 FPR=0, TPR=0
    4. 임계값을 1에서 0으로 낮춰감에 따라 FPR과 TPR은 동시에 증가
    5. FPR이 증가하는 정도보다 TPR이 빠르게 증가하면 이상적(왼쪽 위 꼭지점에 가까울수록 좋음)
    ![Alt text](/reference_machinelearning/image-15.png)

- AUC : ROC 곡선 아래의 면적
    - 가운데 대각선의 직선은 랜덤한 수준의 이진분류에 대응되며 이 경우 AUC는 0.5
    - 1에 가까울수록 좋은 수치
    - FPR이 작을 때 얼마나 큰 TPR을 얻는지에 따라 결정


# Feature Engineering
## 6. 특성 공학 : 개요, 특성 선택 방법론
- 특성공간 차원축소의 필요성
    - 모델의 해석력 향상
    - 모델 훈련시간의 단축
    - 차원의 저주 방지
    - 과적합에 의한 일반화 오차를 줄여 성능 향상

### 특성선택 방법론
- 주어진 특성 변수들 가운데 가장 좋은 특성변수의 조합만 선택
- 불필요한 특성 변수를 제거
![Alt text](/reference_machinelearning/image-16.png)

#### Filter방식
: 각 특성변수를 독립적인 평가함수로 평가
- 각 특성변수 Xi와 목표변수 Y와의 연관성을 측정한 뒤 목표변수를 잘 설명할 수 있는 특성변수만을 선택하는 방식
- Xi와 Y의 1:1관계로만 연관성을 판단
- 연관성 파악을 위해 t-test, chi-square test, information gain 등의 지표 활용

#### Wrapper방식
: 학습 알고리즘을 이용
- 다양한 특성변수의 조합에 대해 목표변수를 예측하기 위한 알고리즘을 훈련하고 corss-validation 등의 방법으로 훈련된 모델의 예측력을 평가, 그 결과를 비교하여 최적화된 특성변수의 조합을 찾는 방법
- 특성변수의 조합이 바뀔때마다 모델을 학습함
- 특성변수에 중복된 정보가 많은 경우 이를 효과적으로 제거
- 순차탐색법인 forward selection, backward selection, stepwise selection 등이 있음
![Alt text](/reference_machinelearning/image-17.png)

#### Embedded방식
: 학습 알고리즘 자체에 feature selection을 포함하는 경우
- Wrapper 방식은 모든 특성변수 조합에 대한 학습을 마친 결과를 비교하는데 비해, Enbedded방식은 학습과정에서 최적화된 변수를 선택한다는 점에서 차이가 있음
- Ridge, Lasso, Elastic net 등


## 7. 특성 공학 : 특성 추출 방법론
- 특성공간 방법론
    - 특성 선택 : 가지고 있는 특성 중 더 유용한 특성을 선택
    - 특성 추출 : 가지고 있는 특성을 결합하여 더 유용한 특성을 선택

### 주성분분석(PCA)
: 서로 연관되어 있는 변수들이 관찰되었을 때, 이 변수들이 전체적으로 가지고 있는 정보들을 최대한 확보하는 적은 수의 새로운 변수(주성분,PC)를 생성하는 방법

- 목적
    - 자료에서 변동이 큰 축을 탐색
    - 변수들에 담긴 정보의 손실을 최소화하면서 차원을 축소
    - 서로 상관이 없거나 독립적인 새로운 변수인 주성분을 통해 데이터의 해석을 용이하게 함

- 기하학적 의미\
: 주성분 축은 원래 변수들의 좌표축이 직교 회전 변환된 것으로 해석가능
    - 첫번째 주성분 축은 데이터의 변동이 가장 커지는 축
    - 두번째 주성분 축은 첫번째 주성분축과 직교하며 첫번째 주성분축 다음으로 데이터의 변동이 큰 축을 나타냄
    - 각 관찰치 별 주성분 점수는 대응하는 원자료값들의 주성분 좌표축에서의 좌표값에 해당
    - 자료들의 공분산 행렬이 대각행렬이 되도록 회전한 것으로 해석 가능
    ![Alt text](/reference_machinelearning/image-18.png)

### 특성값분해(SVD)
- 특이값 분해 : 임의의 n * d 행렬 A는 A = U * (V^T의 합)로 분해가능
    - U와 V는 직교행렬
    - U의 각 열을 A의 왼쪽 특성벡터, V의 각 열을 A의 오른쪽 특성벡터
    - n * d의 대각행렬 : 대각원소를 A의 특성값이라 함

- 주성분분석(PCA)과 특성값분해의 관계
    - A의 오른쪽 특성벡터는 A의 공분산행렬의 고유벡터와 동일
    - 자료행렬에 대한 특성값 분해로 주성분을 도출가능


# Clustering
## 8. 계층적 군집분석
- 군집분석
    - 어떤 개체나 대상들을 밀접한 유사성 또는 비유사성에 의하여 유사한 특성을 지닌 개체들을 몇 개의 군집으로 집단화하는 비지도 학습법
    - 각 군집의 특성, 군집간의 차이 등에 대한 탐색대상으로 집단에 대한 심화된 이해가 목적
    - 특이군집의 발견, 결측값의 보정 등에 사용 가능

- 군집조건
    - 동일 군집에 속한 개체끼리는 유사한 속성이 많음
    - 다른 군집에 속한 개체끼리는 유사한 속성이 적음

### 계층적 군집분석
- 병합적 : 개체간 거리가 가까운 개체끼리 차례로 묶어주는 방법으로 군집을 정의
- 분할적 : 개체간 거리가 먼 개체끼리 나누어 가는 방법으로 군집을 정의
- 계층적 군집분석에서는 병합적 방법이 주로 사용

- 개체간 거리
    - 유클리디안 거리
    - 맨해튼 거리
    - 민코우스키 거리
- 군집간 거리
    - 단일 연결법 : 두 군집 C1과 C2의 거리는 min(d(x, y))
    ![Alt text](/reference_machinelearning/image-19.png)
    - 완전 연결법 : 두 군집 C1과 C2의 거리는 max(d(x, y))
    ![Alt text](/reference_machinelearning/image-20.png)
    - 평균 연결법 : 두 군집의 모든 개체간 거리들의 평균
    ![Alt text](/reference_machinelearning/image-21.png)
    - 중심 연결법 : 두 군집의 중심 사이의 거리
    ![Alt text](/reference_machinelearning/image-22.png)
    - ward 연결법 : SSEk를 군집 k의 중심으로부터 해당 군집간 개체간의 거리 제곱합으로 정의한 뒤 총 K개의 군집이 있다면 SSE = SSEk의 합으로 정의\
    K개 중 2개의 군집을 하나의 군집으로 묶었을 때 오차 제곱합이 증가하는 정도를 두 군집간의 거리로 정의
    ![Alt text](/reference_machinelearning/image-23.png)


## 9. 비계층적 군집분석
### K-평균 군집분석
- 사전에 결정된 군집 수 k에 기초하여 전체 데이터를 상대적으로 유사한 k개의 군집으로 구분
- 계층적 방식에 비해 계산량이 적고 대용량 데이터를 빠르게 처리
- 사전에 적절한 군집수 k에 대한 예상이 필요
- 초기 군집 중심이 어디로 지정되는지에 따라 최종결과가 영향을 많이 받음
- 잡음이나 이상치의 영향을 많이 받음

- K-평균 군집분석 알고리즘
    1. 개체를 k개의 초기 군집으로 나눔
    2. 각 군집의 중심을 계산한 뒤 모든 개체들을 각 군집의 중심에 가장 가까운 군집에 할당
    3. 새로운 개체를 받아들이거나 잃은 군집의 중심을 다시 계산
    4. 위 과정을 더 이상의 재배치가 생기지 않을 때까지 반복
    ![Alt text](/reference_machinelearning/image-24.png)

#### K-평균 군집분석에서 적절한 군집수의 결정
- 오차제곱합(SSE)
    - 각 군집 내 개체들과 해당 군집 중심점과의 거리를 제곱한 값들의 합
    - 오차제곱합이 작을수록 군집내 유사성이 높아 잘 응집된 것

- 군집수 k에 따른 SSE의 변화를 Elbow 차트로 시각화한 뒤 SSE가 급격히 감소하다가 완만해지기 시작하는 시점의 k를 적정 군집수로 판단
![Alt text](/reference_machinelearning/image-25.png)


# Regression
## 10. 단순회귀분석
- 회귀분석 : 독립변수와 종속변수간의 함수적인 관련성을 규명하기 위하여 어떤 수학적 모형을 가정하고 이 모형을 측정된 자료로부터 통계적으로 추정하는 분석방법
- y=f(x)의 함수관계가 있을 때
    - x를 설명변수 또는 독립변수
        - 단순 회귀 : 독립변수 1개
        - 다중 회귀 : 독립변수 2개 이상
    - y를 반응변수 또는 종속변수
![Alt text](/reference_machinelearning/image-26.png)

### 단순선형회귀모형의 평가
- 최소제곱법 : 단순회귀모형에서 자료점과 회귀선 간의 수직거리 제곱합이 최소가 되도록 하는 방법
![Alt text](/reference_machinelearning/image-27.png)

- 결정계수 R^2 = SSR / SST = 1 - (SSE / SST)
- R^2는 두 변수 간의 상관계수 r의 제곱과 같다


## 11~13. 다중회귀분석
- 범주형 독립변수가 포함된 회귀모형 : 더미변수 기법을 사용
    - 더미변수의 개수 = 범주개수 - 1

#### 변수선택 
: 모든 가능한 독립변수들의 조합에 대한 회귀모형을 생성한 뒤 가장 적합한 회귀모형을 선택
- 전진선택법
    - 절편만 있는 모델에서 출발하여 중요한 변수를 하나씩 추가하는 방식
    - 한번 선택된 변수는 제거되지 않는 단점
    ![Alt text](/reference_machinelearning/image-28.png)
- 후진제거법
    - 모든 변수가 포함된 모델에서 가장 중요하지 않은 변수부터 하나씩 제거
    - 한번 제거된 변수는 선택되지 않는 단점
    ![Alt text](/reference_machinelearning/image-29.png)
- 단계선택법
    - 절편만 포함된 모델에서 출발해 가장 중요한 변수부터 추가하고 모델에 포함되어 있는 변수 중에서 중요하지 않은 변수를 제거
    - 더 이상 새롭게 추가되는 변수가 없을때까지 변수의 추가 또는 삭제를 반복

- 모형선택의 기준 
    - 결정계수 R^2는 새로운 독립변수가 추가되면 항상 증가함
    - 이를 보완한 수정결정계수 \
    Adjusted R^2 = {1 - SSE/(n-k-1)} / {SST / (n-1)} 사용

- 잔차분석
    - 오차의 정규성 : 히스토그램, QQ플롯
    ![Alt text](/reference_machinelearning/image-30.png)
    - 오차의 등분산성 : 잔차산점도
    - 오차의 독립성 : 잔차산점도
    ![Alt text](/reference_machinelearning/image-31.png)

- 위반검토 및 해결
    - 오차의 정규성 위반 : 변수 변환
    - 오차의 등분산성 : 가중최소제곱회귀
    - 오차의 독립성 : 시계열 분석

#### 다중공선성
- 독립변수들 간에 강한 선형관계가 존재하는 경우
- 다중회귀모형 분석 시 자주 발생하는 문제 중 하나
- 다중회귀모형에서 회귀계수 추정에 부정적인 영향을 미침
    - 개별적인 회귀계수 추정의 신뢰성이 떨어져 추정치를 믿을수 없게 됨
    - 전반적인 모형의 적합성이나 정확도는 크게 변하지 않음

- VIF계수 도출로 확인 가능 : VIF계수가 5 또는 10이상인 경우 다중공선성이 심각한 것으로 봄
    VIF = 1 / (1 - Rj^2)\
    (Rj^2 : xj 종속변수로 두고 나머지 독립변수로 설명하는 다중선형회귀모델에서의 결정계수)

- 해결책
    - 변수선택으로 중복된 변수 제거
    - 주성분분석 등을 이용해 주옵ㄱ된 변수를 변환하여 새로운 변수 생성
    - 릿지, 라쏘 등으로 중복된 변수의 영향력을 일부만 사용


## 14. 규제가 있는 선형회귀모델
### 선형회귀모델의 규제
: 모형의 과대적합을 막기 위한 규제방법으로 선형회귀모형에서는 보통 모델의 가중치를 제한하는 방법을 사용

- 릿지회귀, 라쏘회귀 모두 추정치는 일반선형회귀모형과 달리 편의가 발생하지만 분산은 더 작아지게 됨

#### 릿지회귀(L2 규제)
: 비용함수에 규제항 람다가 추가된 선형회귀모형
- 람다(규제정도를 결정하는 하이퍼파라미터)
    - 람다가 크면 규제 많음
    - 회귀계수추정치가 작아짐
    - 람다 = 0 이면 일반 선형회귀모델과 동일한 결과

- 제약범위가 각진 형태 : 파라미터의 일부가 0이 되는 경향

#### 라쏘회귀(L1 규제)
: 임의의 람다에 대해 이에 대응하는 하나의 t가 존재하여 동일한 해를 얻게 됨

- 제약범위가 원의 형태 : 파라미터가 0이 되지 않고 전반적으로 줄어드는 경향

#### 엘라스틱 넷
: L1과 L2 규제를 혼합한 방식\
릿지회귀와 라쏘회귀의 장점을 모두 가짐


# Classification
## 15. 분류 : 로지스틱 회귀
: 반응변수가 범주형 데이터인 경우에 사용되는 기법\
새로운 설명변수의 값이 주어질 때 반응변수의 각 범주에 속할 확률이 얼마인지를 추정하고 추정확률을 분류기준값에 따라 분류하는 목적으로 사용

- 이항로지스틱회귀모형 : 이진형값을 가지는 반응변수를 여러 설명변수를 이용하여 회귀식의 형태로 예측하는 모형
![Alt text](/reference_machinelearning/image-32.png)

- 로지스틱회귀모형은 선형의 결정경계를 가짐
![Alt text](/reference_machinelearning/image-33.png)

- 오즈비 : x1만 1만큼 증가하면 성공(관심범주)에 대한 오즈가 exp(b1)배 변화
    - b1 > 0 : 관심범주에 속할 확률이 증가\
    X1 변수와 관심범주간에 양의 상관관계
    - b1 < 0 : 관심범주에 속할 확률이 감소\
    X1 변수와 관심범주간에 음의 상관관계


## 16. 분류 : 나이브베이즈
: 목표변수 Y가 2개의 범주 C1, C2를 가진다고 할 때, 특성변수 X의 값을 이용하여 Y의 범주를 예측하는 문제

- 장점 
    - 데이터의 크기가 커도 연산속도가 빠름
    - 학습에 필요한 데이터 양이 적어도 좋은 성능을 보이는 편
    - 다양한 텍스트 분류나 추천 등에 활용
- 단점
    - Zero frequency문제나 Underflow문제가 있음
    - 모든 독립변수가 독립이라는 가정이 너무 단순함


## 17. 분류 : KNN
: 새로운 데이터가 입력되면 그 새로운 데이터 주변의 가장 가까운 K개의 훈련 데이터의 레이블을 확인한 뒤, 가장 많이 보이는 라벨로 분류하는 방법

- K가 작으면 이상점 등의 노이즈에 민감하게 반응하는 과적합의 문제
- K가 크면 자료의 패턴을 잘 파악할 수 없어 예측 성능이 저하됨

- 거리의 측정 : 자료에 스케일에 차이가 있는 경우 스케일이 큰 특성변수에 의해 거리가 결정되어 버릴 수 있음. 따라서 각 특성변수 별로 스케일이 유사해지도록 표준화변환 또는 min-max변환으로 스케일링을 해준 뒤 거리를 재는 것이 적절


## 18. 의사결정나무모델 : 분류나무
: 의사결정규칙을 나무구조로 도표화하여 관심대상이 되는 집단을 몇 개의 소집단으로 분할하는 방식
- 목표변수가 변수형 : 분류나무
- 목표변수가 연속형 : 회귀나무

- 뿌리마디 : 시작되는 마디로 전체자료로 구성
- 자식마디 : 하나의 마디로부터 분리되어 나간 2개 이상의 마디들
- 부모마디 : 주어진 마디의 상위마디
- 끝마디 : 자식마디가 없는마디
- 중간마디 : 부모마디와 자식마디가 모두 있는 마디
- 깊이 : 뿌리마디부터 끝마디까지 중간마디의 수
![Alt text](/reference_machinelearning/image-34.png)

- 의사결정나무의 종류
![Alt text](/reference_machinelearning/image-35.png)


### 분석절차
- 나무의 성장 : 각 마디에서 적절한 최적의 분리규칙을 찾아 나무를 성장시킴. 정지규칙을 만족하는 경우는 성장을 중단
    - 상위노드로부터 하위노드로 나무 구조를 형성하는 매 단계마다 분리규칙을 선택함
    - 연속형 특성변수 : 분리에 사용될 특성변수 X와 분리점 c를 이용하여 X < c면 왼쪽자식마디, 그렇지 않으면 오른쪽 자식마디로 자료를 정리
    - 범주형 특성변수 : 분리에 사용될 특성변수 X가 가지는 전체범주 중 부분집합인 A를 이용하여 X가 A에 포함되면 왼쪽 자식마디, 그렇지 않으면 오른쪽 자식마디로 자료를 분리
    - 분류기준은 해당노드에서 그 기준으로 하위노드를 분기하였을 때 하위 노드 내에서는 동질성이 하위 노드 간에는 이질성이 가장 커지도록 선택
- 가지치기 : 오류율을 크게 할 위험이 높거나 부적절한 추론규칙을 가지고 있는 가지를 제거
- 타당성 평가 : 평가자료를 이용하여 의사결정나무를 평가
- 해석 및 예측 : 구축된 나무 모형을 해석하고 분류 및 예측모형을 설정

#### 분리규칙
- 불순도가 클수록 자식노드 내 이질성이 큼\
불순도가 가장 작아지는 방향으로 가지분할을 수행
- 모든 특성변수와 그 특성변수의 모든 가능한 분리점에 대하여 G(s, t)를 구한 뒤 G(s, t)가 가장 큰 특성변수 및 분리점을 해당 마디에서의 분리기준으로 정함


## 19. 의사결정나무모델 : 회귀 나무
#### 분리규칙
- 분산의 감소량 : 각 그룹 내에서의 목표변수의 분산이 작을수록, 그룹 내 이질성이 작은 것으로 볼 수 있음\
자식노드로 분리했을 때 분산의 감소량이 가장 커지도록 하는 분리규칙을 탐색

- ANOVA의 F통계량 : F값이 클수록 그룹간에 평균차이가 있다는 것이므로 그룹간 이질성이 큰 것으로 볼 수 있음\
F값이 가장 커지게 되는 분리규칙을 탐색

#### 과적합 방지방법
: 지나치게 많은 마디를 가지는 의사결정나무는 새로운 자료에 적용할 때 예측오차가 커지는 과적합 상태가 됨
- 정지규칙 : 다음의 경우에 더이상 분리하지 않고 나무가 성장을 멈추도록 함
    - 모든 자료의 목표변수값이 동일할 때
    - 마디에 속하는 자료의 개수가 일정 수준보다 적을 때
    - 뿌리마디로부터의 깊이가 일정 수준 이상 일 때
    - 불순도의 감소량이 지정된 값보다 적을 때
- 가지치기
    - 성장이 끝난 나무의 가지를 제거하여 적당한 크기를 가지도록 함
    ![Alt text](/reference_machinelearning/image-36.png)


- 의사결정나무모형 특징
![Alt text](/reference_machinelearning/image-37.png)


# Recommendation
## 20. 추천 : 연관성 분석
- 연관성 규칙을 통해 하나의 거래나 사건에 포함되어 있는 둘 이상의 품목 간 상호 연관성을 발견하는 과정
- 고객이 동시에 구매하는 상품 간의 관계를 분석한다는 의미에서 장바구니분석이라고도 함

 - 연관규칙 : 항목들 간의 if item A -> then item B형태로 표현되는 유용한 패턴

 #### 연관규칙을 파악하기 위한 측도
 - 지지도 : 전체 구매건수 가운데 상품 A와 B를 동시에 구매한 비율(P[A * B])\
    - 지지도(A->B) : A와 B가 동시에 포함된 거래 수 / 전체 거래 수
    - 상품 A 하나에 대한 지지도(A) : A의 거래 수 / 전체 거래 수

- 신뢰도 : 상품 A를 구매한 건 수 가운데 B도 같이 구매한 비율(P[B|A])
    - 신뢰도(A->B) : A와 B가 동시에 포함된 거래 수 / A의 거래 수 = 지지도(A->B) / 지지도(A)


- 향상도 : 전체에서 상품 B를 구매한 비율에 비해 A를 구매한 고객이 B를 구매한 비율이 몇 배ㅣ인가를 의미(P[B|A]/P[B])
    - 향상도(A->B) : 신뢰도(A->B) / 지지도(B)
    - 향상도(A->B) = 1 : 상품 A와 상품 B의 구매는 상호 연관성이 없음
    - 향상도(A->B) > 1 : 상품 A와 상품 B의 구매는 양(+)의 영향력이 있음
    - 향상도(A->B) < 1 : 상품 A와 상품 B의 구매는 음(-)의 영향력이 있음

#### 연역적 알고리즘
- 품목들의 집합 별로 지지도, 신뢰도, 향상도 지표를 구해야 하는데 품목의 수가 많을 때는 연관규칙의 탐색 비용이 크게 증가
- 더 이상 탐색하지 않아도 될 품목의 조합을 찾고 그 조합을 부분집합으로 갖는 품목의 집합들을 가지치기 하여 효율적인 탐색을 하도록 함

- 최소지지도 가지치기 : 어떤 품목(집합)에 대한 지지도가 일정수준을 넘지 못하면 그 품목(집합)이 포함된 조합들은 더 이상 탐색하지 않음